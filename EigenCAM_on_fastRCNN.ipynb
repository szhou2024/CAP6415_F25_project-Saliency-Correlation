{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0478e030",
   "metadata": {},
   "source": [
    "_________________________\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e65cb",
   "metadata": {},
   "source": [
    "Setup and Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482109bf",
   "metadata": {},
   "source": [
    "_________________________\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is your directory path to Salicon images\n",
    "path_to_images = \"...Your path/val/Images_val\"\n",
    "\n",
    "# This is your directory path to ground truth saliency maps\n",
    "path_to_maps = \"...Your path/val/Maps_val\"\n",
    "\n",
    "# This is where you will save results to\n",
    "save_path = \"...Your path/val/backup.npy\"\n",
    "\n",
    "# Turn recovery mode to True if you want to use a backup \n",
    "recovery_mode = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f7a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (this is what I used)\n",
    "!pip install numpy==1.26.4 \\\n",
    "             opencv-python==4.9.0.80 \\\n",
    "             matplotlib==3.8.3 \\\n",
    "             Pillow==10.2.0 \\\n",
    "             scipy==1.12.0 \\\n",
    "             scikit-learn==1.5.2 \\\n",
    "             torch==2.8.0 \\\n",
    "             torchvision==0.23.0 \\\n",
    "             grad-cam==1.5.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import gc\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from pytorch_grad_cam import EigenCAM\n",
    "from pytorch_grad_cam.utils.model_targets import FasterRCNNBoxScoreTarget\n",
    "from pytorch_grad_cam.utils.reshape_transforms import fasterrcnn_reshape_transform\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f48ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEigenCAM(EigenCAM):\n",
    "    def __init__(self, model, target_layers, target_index=0, layer='pool', reshape_transform=None):\n",
    "        # We explicitly pass the required arguments to the parent class\n",
    "        super(MultiEigenCAM, self).__init__(model, \n",
    "                                            target_layers, \n",
    "                                            reshape_transform=self.modified_fasterrcnn_reshape_transform)\n",
    "        self.target_index = target_index\n",
    "        self.S = None\n",
    "        self.rf_weights = None\n",
    "        self.layer = layer\n",
    "\n",
    "    def get_cam_image(self, \n",
    "                      input_tensor,\n",
    "                      target_layer,\n",
    "                      targets,\n",
    "                      activations,\n",
    "                      grads,\n",
    "                      eigen_smooth):\n",
    "        '''\n",
    "        Override inherited method, almost idenital except stores SVD results, \n",
    "        changes projection to specific target_index instead of top 1 PC \n",
    "        '''\n",
    "        \n",
    "        # TBD: use pytorch batch svd implementation\n",
    "        activations[np.isnan(activations)] = 0\n",
    "        projections = []\n",
    "\n",
    "        for a in activations:\n",
    "            reshaped_activations = (a).reshape(\n",
    "                a.shape[0], -1).transpose()\n",
    "            # Centering before the SVD seems to be important here,\n",
    "            # Otherwise the image returned is negative\n",
    "            reshaped_activations = reshaped_activations - \\\n",
    "                reshaped_activations.mean(axis=0)\n",
    "            \n",
    "            # Perform SVD\n",
    "            U, S, VT = np.linalg.svd(reshaped_activations, full_matrices=True)\n",
    "            \n",
    "            self.S = S\n",
    "            self.rf_weights = VT\n",
    "\n",
    "            projection = reshaped_activations @ VT[self.target_index, :]\n",
    "            projection = projection.reshape(a.shape[1:])\n",
    "            projections.append(projection)\n",
    "\n",
    "        return np.float32(projections)\n",
    "    \n",
    "    def calculate_explained_variance(self, numComponents = 1):\n",
    "        '''\n",
    "        Find total % of variance explained by top numComponents \n",
    "        '''\n",
    "\n",
    "        total_explained = 0\n",
    "        squared_s = self.S ** 2\n",
    "\n",
    "        for i in range(0,numComponents):\n",
    "            \n",
    "            # 2. Calculate the ratio for the specific target_index\n",
    "            # Formula: component_variance / total_variance\n",
    "            variance_ratio = squared_s[i] / np.sum(squared_s)\n",
    "            total_explained += variance_ratio\n",
    "\n",
    "        return total_explained\n",
    "    \n",
    "    def modified_fasterrcnn_reshape_transform(self, x):\n",
    "        '''\n",
    "        Override inherited method, everything is the same except\n",
    "        target_size reshapes into specified layer size instead of only \"pool\"\n",
    "        '''\n",
    "\n",
    "        target_size = x[self.layer].size()[-2:]\n",
    "        activations = []\n",
    "        for key, value in x.items():\n",
    "            activations.append(\n",
    "                torch.nn.functional.interpolate(\n",
    "                    torch.abs(value),\n",
    "                    target_size,\n",
    "                    mode='bilinear'))\n",
    "        activations = torch.cat(activations, axis=1)\n",
    "        return activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaliencyMetrics:\n",
    "    \"\"\"\n",
    "    Standard saliency metrics based on MIT Saliency Benchmark\n",
    "    ALL METHODS HERE ARE STATIC, meaning this class is just for ease of namespace\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize(smap):\n",
    "        \"\"\"Normalize to [0, 1]\"\"\"\n",
    "        smap = smap - smap.min()\n",
    "        if smap.max() > 0:\n",
    "            smap = smap / smap.max()\n",
    "        return smap\n",
    "    \n",
    "    @staticmethod\n",
    "    def cc(smap1, smap2):\n",
    "        \"\"\"\n",
    "        Pearson's Correlation Coefficient (CC)\n",
    "        Range: [-1, 1], higher is better\n",
    "        \"\"\"\n",
    "        smap1 = SaliencyMetrics.normalize(smap1).flatten()\n",
    "        smap2 = SaliencyMetrics.normalize(smap2).flatten()\n",
    "        \n",
    "        # Remove mean\n",
    "        smap1 = smap1 - smap1.mean()\n",
    "        smap2 = smap2 - smap2.mean()\n",
    "        \n",
    "        return np.corrcoef(smap1, smap2)[0, 1]\n",
    "\n",
    "    @staticmethod\n",
    "    def cc_mask(smap1, smap2, threshold=1e-4):\n",
    "        \"\"\"\n",
    "        Pearson's Correlation Coefficient (CC)\n",
    "        Range: [-1, 1], higher is better\n",
    "        \n",
    "        Args:\n",
    "            threshold: Values below this are considered 'background/zero'.\n",
    "                       Only pixels active in at least one map are compared.\n",
    "        \"\"\"\n",
    "        # 1. Normalize first so thresholding is consistent (0 to 1)\n",
    "        smap1 = SaliencyMetrics.normalize(smap1).flatten()\n",
    "        smap2 = SaliencyMetrics.normalize(smap2).flatten()\n",
    "        \n",
    "        # 2. Create the Union Mask\n",
    "        # We want to keep pixels where EITHER map 1 OR map 2 is active.\n",
    "        # This removes the \"common zeros\" that inflate correlation.\n",
    "        active_mask = (smap1 > threshold) | (smap2 > threshold)\n",
    "        \n",
    "        # 3. Apply the mask\n",
    "        smap1_active = smap1[active_mask]\n",
    "        smap2_active = smap2[active_mask]\n",
    "        \n",
    "        # 4. Handle Edge Case: If maps are empty (all zeros), return 0\n",
    "        # We need at least 2 points to calculate variance/correlation\n",
    "        if len(smap1_active) < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        # 5. Compute Correlation\n",
    "        return np.corrcoef(smap1_active, smap2_active)[0, 1]\n",
    "    \n",
    "    @staticmethod\n",
    "\n",
    "    def nss(smap, fixation_map):\n",
    "        \"\"\"\n",
    "        Normalized Scanpath Saliency (NSS)\n",
    "        Range: [-∞, ∞], higher is better, >1 is good\n",
    "        \n",
    "        Args:\n",
    "            smap: predicted saliency map\n",
    "            fixation_map: binary or continuous fixation map\n",
    "        \"\"\"\n",
    "        smap_normalized = (smap - smap.mean()) / (smap.std() + 1e-8)\n",
    "        fixation_points = fixation_map > 0\n",
    "        \n",
    "        if not fixation_points.any():\n",
    "            return 0.0\n",
    "        \n",
    "        return smap_normalized[fixation_points].mean()\n",
    "    \n",
    "    # def nss(smap, fixation_map):\n",
    "    #     \"\"\"\n",
    "    #     Normalized Scanpath Saliency (NSS)\n",
    "    #     Range: [-∞, ∞], higher is better, >1 is good\n",
    "        \n",
    "    #     Args:\n",
    "    #         smap: predicted saliency map\n",
    "    #         fixation_map: binary or continuous fixation map\n",
    "    #     \"\"\"\n",
    "    #     # Normalize saliency map to have zero mean and unit std\n",
    "    #     smap_normalized = (smap - smap.mean()) / (smap.std() + 1e-8)\n",
    "        \n",
    "    #     # Get fixation locations\n",
    "    #     if fixation_map.max() > 0:\n",
    "    #         fixation_map = fixation_map / fixation_map.max()\n",
    "        \n",
    "    #     # Compute NSS\n",
    "    #     nss_value = (smap_normalized * fixation_map).sum() / (fixation_map.sum() + 1e-8)\n",
    "        \n",
    "    #     return nss_value\n",
    "    \n",
    "    @staticmethod\n",
    "    def auc_judd(smap, fixation_map, num_splits=100):\n",
    "        \"\"\"\n",
    "        AUC Judd (Area Under ROC Curve)\n",
    "        Range: [0, 1], higher is better, 0.5 is chance\n",
    "        \n",
    "        Treats fixated pixels as positive, all others as negative\n",
    "        \"\"\"\n",
    "        smap_flat = smap.flatten()\n",
    "        fixation_flat = (fixation_map > 0).astype(int).flatten()\n",
    "        \n",
    "        # Need both positive and negative samples\n",
    "        if len(np.unique(fixation_flat)) < 2:\n",
    "            return np.nan\n",
    "        \n",
    "        try:\n",
    "            return roc_auc_score(fixation_flat, smap_flat)\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    @staticmethod\n",
    "    def auc_shuffled(smap, fixation_map, other_maps=None, num_splits=100):\n",
    "        \"\"\"\n",
    "        AUC Shuffled (using other images as negative samples)\n",
    "        More realistic than AUC Judd\n",
    "        \"\"\"\n",
    "        # Simplified version - use threshold-based approach\n",
    "        smap_flat = smap.flatten()\n",
    "        \n",
    "        # Positive samples: actual fixations\n",
    "        pos_indices = fixation_map.flatten() > 0\n",
    "        \n",
    "        if pos_indices.sum() == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # Negative samples: non-fixated locations\n",
    "        neg_indices = ~pos_indices\n",
    "        \n",
    "        if neg_indices.sum() == 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # Create binary labels\n",
    "        y_true = np.concatenate([np.ones(pos_indices.sum()), \n",
    "                                 np.zeros(min(neg_indices.sum(), pos_indices.sum() * 10))])\n",
    "        y_scores = np.concatenate([smap_flat[pos_indices],\n",
    "                                   np.random.choice(smap_flat[neg_indices], \n",
    "                                                   min(neg_indices.sum(), pos_indices.sum() * 10))])\n",
    "        \n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_scores)\n",
    "        except:\n",
    "            return np.nan\n",
    "    \n",
    "    @staticmethod\n",
    "    def kld(smap, fixation_map):\n",
    "        \"\"\"\n",
    "        Kullback-Leibler Divergence (KLD)\n",
    "        Range: [0, ∞], lower is better\n",
    "        \"\"\"\n",
    "        # Convert to probability distributions\n",
    "        smap = SaliencyMetrics.normalize(smap)\n",
    "        fixation_map = SaliencyMetrics.normalize(fixation_map)\n",
    "        \n",
    "        smap_flat = smap.flatten()\n",
    "        fix_flat = fixation_map.flatten()\n",
    "        \n",
    "        # Convert to distributions\n",
    "        eps = 1e-8\n",
    "        smap_prob = smap_flat / (smap_flat.sum() + eps)\n",
    "        fix_prob = fix_flat / (fix_flat.sum() + eps)\n",
    "        \n",
    "        # Avoid log(0)\n",
    "        smap_prob = np.maximum(smap_prob, eps)\n",
    "        fix_prob = np.maximum(fix_prob, eps)\n",
    "        \n",
    "        return np.sum(fix_prob * np.log(fix_prob / smap_prob))\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_all_metrics(predicted, ground_truth):\n",
    "        \"\"\"\n",
    "        Compute all metrics at once\n",
    "        \n",
    "        Args:\n",
    "            predicted: predicted saliency map (H, W)\n",
    "            ground_truth: ground truth saliency/fixation map (H, W)\n",
    "        \n",
    "        Returns:\n",
    "            dict of metrics\n",
    "        \"\"\"\n",
    "        # Ensure same size\n",
    "        if predicted.shape != ground_truth.shape:\n",
    "            predicted = cv2.resize(predicted, \n",
    "                                  (ground_truth.shape[1], ground_truth.shape[0]))\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "\n",
    "        try:\n",
    "            metrics['CC_mask'] = SaliencyMetrics.cc_mask(predicted, ground_truth)\n",
    "        except:\n",
    "            metrics['CC_mask'] = np.nan\n",
    "        \n",
    "        try:\n",
    "            metrics['CC_standard'] = SaliencyMetrics.cc(predicted, ground_truth)\n",
    "        except:\n",
    "            metrics['CC_standard'] = np.nan\n",
    "        \n",
    "        try:\n",
    "            metrics['NSS'] = SaliencyMetrics.nss(predicted, ground_truth)\n",
    "        except:\n",
    "            metrics['NSS'] = np.nan\n",
    "        \n",
    "        try:\n",
    "            metrics['AUC-Judd'] = SaliencyMetrics.auc_judd(predicted, ground_truth)\n",
    "        except:\n",
    "            metrics['AUC-Judd'] = np.nan\n",
    "        \n",
    "        try:\n",
    "            metrics['KLD'] = SaliencyMetrics.kld(predicted, ground_truth)\n",
    "        except:\n",
    "            metrics['KLD'] = np.nan\n",
    "        \n",
    "        # Additional simple metrics\n",
    "        pred_norm = SaliencyMetrics.normalize(predicted)\n",
    "        gt_norm = SaliencyMetrics.normalize(ground_truth)\n",
    "        \n",
    "        metrics['MAE'] = np.mean(np.abs(pred_norm - gt_norm))\n",
    "        metrics['MSE'] = np.mean((pred_norm - gt_norm) ** 2)\n",
    "        \n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1738da47",
   "metadata": {},
   "source": [
    "_________________________\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f915e28",
   "metadata": {},
   "source": [
    "Main Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758acaf1",
   "metadata": {},
   "source": [
    "_________________________\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b55f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cams(image_loc, saliency_path, title, orig_cor = -100.00, CAMS=[], output_saliency_metrics = False, output_display = False):\n",
    "    \n",
    "    image = np.array(Image.open(image_loc))\n",
    "    image_float_np = np.float32(image) / 255\n",
    "\n",
    "    # Transform image into a tensor\n",
    "    \n",
    "    # Construct Model\n",
    "    # Define the torchvision image transforms\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "    input_tensor = transform(image)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # Add a batch dimension:\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Declare model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # All FPN layers\n",
    "    target_layers = [model.backbone]\n",
    "\n",
    "    # Make a custom eigenCAM\n",
    "    cam = MultiEigenCAM(model,\n",
    "                target_layers,\n",
    "                target_index=0,\n",
    "                reshape_transform=fasterrcnn_reshape_transform)\n",
    "\n",
    "    # Not actually used for EigenCAM, only for gradient based\n",
    "    targets = [FasterRCNNBoxScoreTarget(labels=[], bounding_boxes=[])]\n",
    "\n",
    "    # Produce saliency map\n",
    "    with torch.no_grad():  # Avoid unnecessary gradient computation\n",
    "        grayscale_cam = cam(input_tensor, targets=targets)\n",
    "\n",
    "    # Take the first image in the batch (there's just only one in this implementation)\n",
    "    grayscale_cam = grayscale_cam[0, :] # Take all rows and columns\n",
    "\n",
    "    # Apply \"Fairness\" Blur (sigma = 19)\n",
    "    # This converts the \"Blocky/Grid\" activation into a \"Probability\" distribution matching the SALICON style.\n",
    "    smooth_cam = gaussian_filter(grayscale_cam, sigma=19)\n",
    "\n",
    "    # Normalize again (optional but recommended for visualization/metrics)\n",
    "    smooth_cam = smooth_cam - np.min(smooth_cam)\n",
    "    smooth_cam = smooth_cam / (np.max(smooth_cam) + 1e-7)\n",
    "\n",
    "    # Load the ground truth saliency map (PNG), Normalize to [0, 1]\n",
    "    gt_saliency_np = np.array(Image.open(saliency_path)).astype(np.float32) / 255.0\n",
    "\n",
    "    # Append to CAMS if output_saliency_metrics = True\n",
    "    if output_saliency_metrics == True:\n",
    "\n",
    "        CAMS.append(SaliencyMetrics.compute_all_metrics(grayscale_cam, gt_saliency_np))\n",
    "        CAMS.append(SaliencyMetrics.compute_all_metrics(smooth_cam, gt_saliency_np))\n",
    "\n",
    "    if output_display == True:\n",
    "\n",
    "        # Overlay saliency map on original image\n",
    "        cam_image = show_cam_on_image(image_float_np, grayscale_cam, use_rgb=True)\n",
    "        smooth_cam_image = show_cam_on_image(image_float_np, smooth_cam, use_rgb=True)\n",
    "        gt_map = show_cam_on_image(image_float_np, gt_saliency_np, use_rgb=True)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(18, 6))\n",
    "\n",
    "        fig.suptitle(f\"Heatmaps for Image ID: {title} \\n Correlation: {orig_cor:.02}\", fontsize=20)\n",
    "\n",
    "        axes[0].imshow(image_float_np)\n",
    "        axes[0].set_title('Original Image', fontsize=14)\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        axes[1].imshow(cam_image)\n",
    "        axes[1].set_title('FastRCNN + CAM', fontsize=14)\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        axes[2].imshow(smooth_cam_image)\n",
    "        axes[2].set_title('SMOOTHED CAM', fontsize=14)\n",
    "        axes[2].axis('off')\n",
    "\n",
    "        axes[3].imshow(gt_map)\n",
    "        axes[3].set_title('Ground Truth', fontsize=14)\n",
    "        axes[3].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    # Delete cam\n",
    "    del cam   \n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae096ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(main_dict, keys=[]):\n",
    "    '''\n",
    "    Pass this the results obtained, display output graphs row by row\n",
    "    '''\n",
    "\n",
    "    for key in keys:\n",
    "    \n",
    "        # Extract info from keys\n",
    "        generate_cams(main_dict[key[0]][0], main_dict[key[0]][1], title=key[0][-12:], orig_cor=key[1], output_display=True)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a92205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Loop (skip if recovery mode)\n",
    "\n",
    "if recovery_mode == False:\n",
    "\n",
    "    # Dictionary of path pairs, image + ground truth saliency map, as well as all outputs of processing\n",
    "    main_dict = {}\n",
    "\n",
    "    # Loop through source image list\n",
    "    for filename in os.listdir(path_to_images): \n",
    "        \n",
    "        # Make a dict key name if doesn't exist\n",
    "        dict_key = filename[:-4]\n",
    "        if (dict_key in main_dict) == False:\n",
    "            main_dict[dict_key] = [os.path.join(path_to_images, filename)]\n",
    "\n",
    "    # Loop through ground truth list\n",
    "    for filename in os.listdir(path_to_maps):\n",
    "\n",
    "        # Check corresponding source image\n",
    "        dict_key = filename[:-4]\n",
    "        if (dict_key in main_dict):\n",
    "            main_dict[dict_key].append(os.path.join(path_to_maps, filename))\n",
    "\n",
    "    # This should be 5000 images if done properly\n",
    "    print(f\"Total # of images: {len(main_dict)}\")\n",
    "\n",
    "    # Main Loop\n",
    "    count = 0\n",
    "    keys = []\n",
    "\n",
    "    for key in main_dict:\n",
    "        \n",
    "        image_loc = main_dict[key][0]\n",
    "        saliency_path = main_dict[key][1]\n",
    "        CAMS = [] # Empty cams\n",
    "        title = key[-12:]\n",
    "\n",
    "        # Returns a list of source image + CAMS \n",
    "        # By default, CAM objects will have Principal Component Metrics already\n",
    "        # Set output_saliency_metrics=True to get metrics when comparing to ground truth \n",
    "        # Set output_display=True to display side by side heatmaps w/ original image\n",
    "        generate_cams(image_loc, saliency_path, title, CAMS=CAMS, output_saliency_metrics=True, output_display=False)\n",
    "        main_dict[key].append(CAMS)\n",
    "        keys.append([key, main_dict[key][-1][-1][\"CC_mask\"], main_dict[key][-1][-1][\"CC_standard\"], main_dict[key][-1][-1][\"NSS\"]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5503b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a backup file (VERY IMPORTANT, otherwise you'll run another 6+ hours)\n",
    "\n",
    "if recovery_mode == False:\n",
    "    \n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok = True)  # Create dirs if needed\n",
    "    np.save(save_path, main_dict, allow_pickle=True)\n",
    "\n",
    "else:\n",
    "    \n",
    "    # To load it back:\n",
    "    main_dict = np.load(save_path, allow_pickle=True).item()\n",
    "    keys = []\n",
    "    for key in main_dict:\n",
    "        keys.append([key, main_dict[key][-1][-1][\"CC_mask\"], main_dict[key][-1][-1][\"CC_standard\"], main_dict[key][-1][-1][\"NSS\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e419ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top, middle, and bottom n images (default n = 25, change is needed)\n",
    "n = 25\n",
    "\n",
    "# Display most correlated images\n",
    "top_sorted_keys = sorted(keys, key=lambda x: x[2], reverse=True)\n",
    "display_images(main_dict, top_sorted_keys[0:n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bcbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display images with near 0 correlation\n",
    "\n",
    "# Display least correlated images\n",
    "bot_sorted_keys = sorted(keys, key=lambda x: x[1])\n",
    "display_images(main_dict, bot_sorted_keys[2500:2500+n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefe14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display least correlated images\n",
    "bot_sorted_keys = sorted(keys, key=lambda x: x[2])\n",
    "display_images(main_dict, bot_sorted_keys[0:n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary metrics across all 5000 images\n",
    "\n",
    "cc_values = [k[1] for k in keys]\n",
    "\n",
    "# Create decile bins\n",
    "deciles = np.percentile(cc_values, np.arange(0, 101, 10))\n",
    "\n",
    "# Assign each value to a decile and compute averages\n",
    "decile_averages = []\n",
    "decile_counts = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Get values in this decile range\n",
    "    lower = deciles[i]\n",
    "    upper = deciles[i + 1]\n",
    "    \n",
    "    # Include upper bound for last decile\n",
    "    if i == 9:\n",
    "        bucket_values = [v for v in cc_values if lower <= v <= upper]\n",
    "    else:\n",
    "        bucket_values = [v for v in cc_values if lower <= v < upper]\n",
    "    \n",
    "    decile_averages.append(np.mean(bucket_values) if bucket_values else 0)\n",
    "    decile_counts.append(len(bucket_values))\n",
    "\n",
    "# Step 4: Display results\n",
    "print(\"Decile Analysis of CC_mask:\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(10):\n",
    "    print(f\"Decile {i+1:2d} [{deciles[i]:6.3f} - {deciles[i+1]:6.3f}]: \"\n",
    "          f\"avg = {decile_averages[i]:.4f}, count = {decile_counts[i]:4d}\"\n",
    "          f\" \")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Overall average: {np.mean(cc_values):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f0f47",
   "metadata": {},
   "source": [
    "_________________________\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b00ec69",
   "metadata": {},
   "source": [
    "Deeper Dive into Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3c5cf9",
   "metadata": {},
   "source": [
    "_________________________\n",
    "_________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_dive(image_loc, saliency_path, title, orig_cor, CAMS=[], invert=False):\n",
    "\n",
    "    image = np.array(Image.open(image_loc))\n",
    "    image_float_np = np.float32(image) / 255\n",
    "\n",
    "    # Transform image into a tensor\n",
    "\n",
    "    # Construct Model\n",
    "    # Define the torchvision image transforms\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    input_tensor = transform(image)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    # Add a batch dimension:\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Declare model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    target_layers = [model.backbone]\n",
    "\n",
    "    # Load the ground truth saliency map (PNG), Normalize to [0, 1]\n",
    "    gt_saliency_np = np.array(Image.open(saliency_path)).astype(np.float32) / 255.0\n",
    "\n",
    "    gt_map = show_cam_on_image(image_float_np, gt_saliency_np, use_rgb=True)\n",
    "\n",
    "    # Make EigenCAM maps for 3 spatial dimensions\n",
    "    cor_after = -100.00 # This is the max of FPN layer 2 and layer 3 dimensions\n",
    "    layer_after = 0\n",
    "\n",
    "    for i in range (2,5):\n",
    "\n",
    "        if i != 4:\n",
    "            cam = MultiEigenCAM(model,\n",
    "                        target_layers,\n",
    "                        layer=str(i),\n",
    "                        target_index=0,\n",
    "                        reshape_transform=fasterrcnn_reshape_transform)\n",
    "            \n",
    "        else:\n",
    "            cam = MultiEigenCAM(model,\n",
    "                        target_layers,\n",
    "                        layer='pool',\n",
    "                        target_index=0,\n",
    "                        reshape_transform=fasterrcnn_reshape_transform)\n",
    "    \n",
    "        # Not actually used for EigenCAM, only for gradient based\n",
    "        targets = [FasterRCNNBoxScoreTarget(labels=[], bounding_boxes=[])]\n",
    "\n",
    "        # Produce saliency map\n",
    "        with torch.no_grad():  # Avoid unnecessary gradient computation\n",
    "            grayscale_cam = cam(input_tensor, targets=targets)\n",
    "\n",
    "        # Take the first image in the batch (there's just only one in this implementation)\n",
    "        grayscale_cam = grayscale_cam[0, :] # Take all rows and columns\n",
    "\n",
    "        # Do an experimental flip of signs to get \"correct\" signs\n",
    "        if i == 4 and invert == True:\n",
    "            grayscale_cam = -grayscale_cam\n",
    "\n",
    "        # Apply \"Fairness\" Blur (sigma = 19)\n",
    "        # This converts the \"Blocky/Grid\" activation into a \"Probability\" distribution matching the SALICON style.\n",
    "        smooth_cam = gaussian_filter(grayscale_cam, sigma=19)\n",
    "\n",
    "        # Normalize again (optional but recommended for visualization/metrics)\n",
    "        smooth_cam = smooth_cam - np.min(smooth_cam)\n",
    "        smooth_cam = smooth_cam / (np.max(smooth_cam) + 1e-7)\n",
    "\n",
    "        # Overlay saliency map on original image\n",
    "        smooth_cam_image = show_cam_on_image(image_float_np, smooth_cam, use_rgb=True)\n",
    "        CAMS.append(smooth_cam_image)\n",
    "\n",
    "        # Check which correlation is larger, replace if needed\n",
    "\n",
    "        cc = SaliencyMetrics.cc_mask(smooth_cam, gt_saliency_np)\n",
    "        if cor_after < cc:\n",
    "            cor_after = cc\n",
    "            layer_after = str(i) \n",
    "\n",
    "    if layer_after == '2':\n",
    "        layer_after = f\"(FPN layer {layer_after}, 30x40)\"\n",
    "    elif layer_after == '3':\n",
    "        layer_after = f\"(FPN layer {layer_after}, 15x20)\"\n",
    "    else:\n",
    "        layer_after = f\"(FPN pool layer, 8x10)\"\n",
    "\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(18, 6))\n",
    "\n",
    "    fig.suptitle(f\"Spatial Comparison Image ID: {title} \\n Default Cor: {orig_cor:.02} ------ Cor After: {cor_after:.02} {layer_after}\", fontsize=20)\n",
    "\n",
    "    axes[0].imshow(image_float_np)\n",
    "    axes[0].set_title('Original Image', fontsize=14)\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(CAMS[-1])\n",
    "    axes[1].set_title('8x10 (Pool Flipped)', fontsize=14)\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(CAMS[-2])\n",
    "    axes[2].set_title('15x20 (Blurring)', fontsize=14)\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    axes[3].imshow(CAMS[-3])\n",
    "    axes[3].set_title('30x40 (Normal)', fontsize=14)\n",
    "    axes[3].axis('off')\n",
    "\n",
    "    axes[4].imshow(gt_map)\n",
    "    axes[4].set_title('Ground Truth', fontsize=14)\n",
    "    axes[4].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "    # Delete cam\n",
    "    del cam   \n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656fe615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a deeper dive into bottom n by changing spatial dimensions\n",
    "\n",
    "# Top n images to look at\n",
    "n = 25\n",
    "\n",
    "# Display least correlated images\n",
    "\n",
    "for i in range(0, n):\n",
    "    \n",
    "    bot_sorted_keys = sorted(keys, key=lambda x: x[2])\n",
    "    image_loc = main_dict[bot_sorted_keys[i][0]][0]\n",
    "    saliency_path = main_dict[bot_sorted_keys[i][0]][1]\n",
    "    title = bot_sorted_keys[i][0][-12:]\n",
    "    orig_cor = main_dict[bot_sorted_keys[i][0]][-1][-1]['CC_mask']\n",
    "\n",
    "    deep_dive(image_loc, saliency_path, title, orig_cor, invert=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
