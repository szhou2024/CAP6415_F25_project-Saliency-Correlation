Decided on using SALICON dataset, still deciding on which particular subset. I'm thinking validation is best, since it's only 5000 images and has ground truths, whereas test set doesn't have.

Still researching which model + CAM type to use as the representative "gaze."

Looked into using DeepGaze models, but that's not going to produce very interesting analysis because it's SOTA and very accurate on SALICON. I think I'm going to explore a more interesting analysis for my purposes. Additionally, DeepGaze is quite computationally expensive, so I'm going to shy away from that.

Formulated a plan: I'm going with an unorthodox comparison of "instant/quick gaze" with the ground truths in SALICON. I expect this to have fairly high discrepancies and low correlation for the majority of images. Perfect for what I want to explore though.

Modelwise, I want a relatively small network, preferably with something that uses downsampling heavily. Similarly, I want a CAM method that simulates "first impressions," not really a calculated decision making process.

Stumbled upon what I think could potentially be a great representation of what I'm looking for. Going to explore this more. Link below for the author of the widely used grad-cam library:
https://github.com/jacobgil/pytorch-grad-cam/tree/master

Examples/tutorials are incredibly informative and intuitive. Looking into faster Grad-Cams.

Looked at the following 2 and realized that EigenCAM is potentially perfect:

https://github.com/jacobgil/pytorch-grad-cam/blob/master/tutorials/Class%20Activation%20Maps%20for%20Object%20Detection%20With%20Faster%20RCNN.ipynb

https://github.com/jacobgil/pytorch-grad-cam/blob/master/tutorials/EigenCAM%20for%20YOLO5.ipynb




